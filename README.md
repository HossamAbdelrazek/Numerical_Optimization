# Numerical Optimization from Scratch

This repository contains a collection of Jupyter notebooks that implement key numerical optimization algorithms from scratch. Each notebook provides both theoretical explanations and practical implementations, aimed at deepening understanding of how these methods work under the hood.

## üìò Contents

The following optimization techniques are covered:

- **Linear Regression**
  - Single-variable
  - Multi-variable

- **Gradient Descent Variants**
  - Batch Gradient Descent
  - Stochastic Gradient Descent (SGD)
  - Mini-Batch Gradient Descent
  - Momentum
  - Nesterov Accelerated Gradient (NAG)

- **Adaptive Optimization Algorithms**
  - Adagrad
  - RMSProp
  - Adam

- **Second-Order Methods**
  - Newton‚Äôs Method
  - BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno)

## üß† Objectives

- Implement core optimization algorithms from scratch without relying on high-level libraries.
- Understand the mathematical intuition and mechanics behind each method.
- Visualize and compare the convergence behaviors of different optimization techniques.

## üõ†Ô∏è Technologies Used

- **Python** (3.8+)
- **NumPy**, **Matplotlib**, **Seaborn** ‚Äì for numerical computation and visualization
- **Jupyter Notebook** ‚Äì for interactive development and demonstration

## üöÄ Getting Started

To run the notebooks:

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/numerical-optimization-from-scratch.git
   cd numerical-optimization-from-scratch
   ```
2. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```
3. Launch Jupyter:
   ```bash
   jupyter notebook
   ```
## Example Visualizations
- Each notebook includes detailed plots showing the optimization path, convergence behavior, and comparison between different methods.
